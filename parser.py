import sqlite3
import requests
import time
import random
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = "https://bygame.ru"
START_PAGE = "https://bygame.ru/otvety/wow"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36"
}

PROGRESS_DB = "progress.db"  # –¥–ª—è —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —É—Ä–æ–≤–Ω—è–º–∏
LEVELS_DB = "levels.db"      # –¥–ª—è —Å–∞–º–∏—Ö —É—Ä–æ–≤–Ω–µ–π

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º ---
def init_progress_db():
    conn = sqlite3.connect(PROGRESS_DB)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS pages (
            url TEXT PRIMARY KEY,
            processed BOOLEAN NOT NULL DEFAULT 0
        )
    """)
    conn.commit()
    conn.close()

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã —Å —É—Ä–æ–≤–Ω—è–º–∏ ---
def init_levels_db():
    conn = sqlite3.connect(LEVELS_DB)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS levels (
            level INTEGER PRIMARY KEY,
            main_words TEXT NOT NULL,
            bonus_words TEXT
        )
    """)
    conn.commit()
    conn.close()

# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ progress.db ---
def save_links(urls):
    conn = sqlite3.connect(PROGRESS_DB)
    c = conn.cursor()
    for url in urls:
        try:
            c.execute("INSERT OR IGNORE INTO pages (url) VALUES (?)", (url,))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ —Å—Å—ã–ª–∫–∏ {url}: {e}")
    conn.commit()
    conn.close()

# --- –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---
def get_unprocessed_links():
    conn = sqlite3.connect(PROGRESS_DB)
    c = conn.cursor()
    c.execute("SELECT url FROM pages WHERE processed = 0")
    rows = c.fetchall()
    conn.close()
    return [row[0] for row in rows]

# --- –û—Ç–º–µ—á–∞–µ–º —Å—Å—ã–ª–∫—É –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é ---
def mark_processed(url):
    conn = sqlite3.connect(PROGRESS_DB)
    c = conn.cursor()
    c.execute("UPDATE pages SET processed = 1 WHERE url = ?", (url,))
    conn.commit()
    conn.close()

# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Ä–æ–≤–Ω–∏ –≤ levels.db ---
def save_level(level_num, main_words, bonus_words):
    conn = sqlite3.connect(LEVELS_DB)
    c = conn.cursor()
    c.execute("""
        INSERT OR REPLACE INTO levels (level, main_words, bonus_words) VALUES (?, ?, ?)
    """, (level_num, ",".join(main_words), ",".join(bonus_words) if bonus_words else None))
    conn.commit()
    conn.close()

# --- –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
def get_all_links():
    print("üîé –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —É—Ä–æ–≤–Ω—è–º–∏...")
    r = requests.get(START_PAGE, headers=HEADERS)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    links = []
    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /otvety/wow-...
    for a in soup.select("li > a.uk-button"):
        href = a.get("href", "")
        if href.startswith("/otvety/wow-"):
            full_url = BASE_URL + href
            links.append(full_url)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫.")
    return links

# --- –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —É—Ä–æ–≤–Ω—è–º–∏ ---
def parse_level_page(url):
    print(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
    r = requests.get(url, headers=HEADERS)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    results = []
    headers = soup.find_all("h2", class_="uk-h3")
    for h2 in headers:
        title = h2.get_text(strip=True)
        if not title.startswith("–£—Ä–æ–≤–µ–Ω—å"):
            continue
        try:
            level_num = int(title.split()[1])
        except:
            continue

        # –°–ª–µ–¥—É—é—â–∏–π sibling <p> —Å –æ—Ç–≤–µ—Ç–∞–º–∏
        p = h2.find_next_sibling("p")
        if not p:
            continue

        # –í p –µ—Å—Ç—å —Å–∏–ª—å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ <strong> - –º–æ–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–¥—É—Ç —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ <strong> (—Ç–µ–∫—Å—Ç), –±–æ–Ω—É—Å–Ω—ã–µ - –≤ <span>
        strong = p.find("strong")
        # –¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ strong (–∏ <br>)
        text_nodes = []
        for elem in strong.next_siblings:
            if elem.name == "br":
                continue
            if isinstance(elem, str):
                text_nodes.append(elem.strip())
            elif elem.name == "span":
                # bonus
                continue
        main_words_text = " ".join(text_nodes).replace("\n", " ").replace(",", " ")
        main_words = [w.strip().upper() for w in main_words_text.split() if w.strip()]

        # –±–æ–Ω—É—Å–Ω—ã–µ —Å–ª–æ–≤–∞
        bonus_words = []
        span = p.find("span", class_="uk-text-meta")
        if span:
            bonus_text = span.get_text(separator=",").replace("\n", " ")
            bonus_words = [w.strip().upper() for w in bonus_text.split(",") if w.strip()]

        results.append((level_num, main_words, bonus_words))
    return results

def worker(url):
    try:
        levels = parse_level_page(url)
        for level_num, main_words, bonus_words in levels:
            save_level(level_num, main_words, bonus_words)
        mark_processed(url)
        time.sleep(random.uniform(0.5, 1.5))
        return True, url
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
        return False, url

def main():
    init_progress_db()
    init_levels_db()

    # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
    conn = sqlite3.connect(PROGRESS_DB)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM pages")
    count = c.fetchone()[0]
    conn.close()

    if count == 0:
        links = get_all_links()
        save_links(links)

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    try:
        threads = int(input("–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (–º–∞–∫—Å 10): "))
        if threads < 1:
            threads = 1
        elif threads > 10:
            threads = 10
    except:
        threads = 5
    print(f"–ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä —Å {threads} –ø–æ—Ç–æ–∫–∞–º–∏...")

    links_to_process = get_unprocessed_links()
    print(f"–°—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(links_to_process)}")

    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = {executor.submit(worker, url): url for url in links_to_process}
        for future in as_completed(futures):
            success, url = future.result()
            if success:
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {url}")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞: {url}")

if __name__ == "__main__":
    main()
